# -*- coding: utf-8 -*-
"""SuggestionCurrent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXdqsnhG6UjiiHmLFLrNPWquCInD7Yiu
"""

from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! kaggle datasets list -s emotions
! kaggle datasets download -d praveengovi/emotions-dataset-for-nlp
! unzip emotions-dataset-for-nlp

'''
Autosuggested words based on letter, suggested row on UI
'''

# Preprocessing
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import keras.utils.np_utils as ku
import tensorflow as tf

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer, sent_tokenize
from gensim.models import Word2Vec

# ML
from keras.models import Model, load_model
from keras.layers import Bidirectional, LSTM, Embedding, Dense, Dropout, GRU, Flatten
from keras.callbacks import ModelCheckpoint
from keras import Input
from keras.regularizers import l2
from tensorflow.keras.optimizers import RMSprop

# Misc libraries
import matplotlib.pyplot as plt
import numpy as np
import os
import gc

gc.enable()

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

MAX_SAMPLES = 40000

'''
Load text
'''

# text = open('./train.txt').read().lower().split('\n')[0:6000]
# path = tf.keras.utils.get_file('nietzsche.txt',
#                                origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')

# text = open(path).read().lower()
# # with open(path) as f:
# #   text = f.readlines()

# path_to_zip = tf.keras.utils.get_file(
#     "cornell_movie_dialogs.zip",
#     origin="http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip",
#     extract=True,
# )

# path_to_dataset = os.path.join(
#     os.path.dirname(path_to_zip), "cornell movie-dialogs corpus"
# )
# path_to_movie_lines = os.path.join(path_to_dataset, "movie_lines.txt")
# path_to_movie_conversations = os.path.join(path_to_dataset, "movie_conversations.txt")


# def load_conversations():
#     # Helper function for loading the conversation splits
#     id2line = {}
#     with open(path_to_movie_lines, errors="ignore") as file:
#         lines = file.readlines()
#     for line in lines:
#         parts = line.replace("\n", "").split(" +++$+++ ")
#         id2line[parts[0]] = parts[4]

#     inputs, outputs = [], []
#     with open(path_to_movie_conversations, "r") as file:
#         lines = file.readlines()
#     for line in lines:
#         parts = line.replace("\n", "").split(" +++$+++ ")
#         # get conversation in a list of line ID
#         conversation = [line[1:-1] for line in parts[3][1:-1].split(", ")]
#         for i in range(len(conversation) - 1):
#             inputs.append(id2line[conversation[i]])
#             outputs.append(id2line[conversation[i + 1]])
#             if len(inputs) >= MAX_SAMPLES:
#                 return inputs, outputs
#     return inputs, outputs


# questions, answers = load_conversations()

# print(len(questions))
# text = ' '.join(answers)
# print(text)

text = open('./1661-0.txt').read().lower()
print(text)

tokenizer = RegexpTokenizer(r'\w+')
words = tokenizer.tokenize(text.lower())

# stop_words = set(stopwords.words('english'))

# tokenizer = RegexpTokenizer(r'\w+')
# words = tokenizer.tokenize(text.lower())

# print(stop_words)

# text = [i for i in words if not i in stop_words]

# text = ' '.join(text)
# print(text[0:100])

# constants
MAX_LEN = 5
STEP = 3

sentences, next_char = [], []

for i in range(len(text) - MAX_LEN):
  sentences.append(text[i: i + MAX_LEN])
  next_char.append(text[i + MAX_LEN])

print('sentences[0]', sentences[0])
print('next_char[0]', next_char[0])
print('sentences[1]', sentences[1])
print('next_char[1]', next_char[1])

char_lst = sorted(list(set(text)))
char_dict = {char: char_lst.index(char) for char in char_lst}

print(char_lst)
print(len(char_lst))

X = np.zeros((len(sentences), MAX_LEN, len(char_lst)), dtype=np.bool)
y = np.zeros((len(next_char), len(char_lst)), dtype=np.bool)

for i, sentence in enumerate(sentences):
  for j, char in enumerate(sentence):
    X[i, j, char_dict[char]] = 1
  y[i, char_dict[next_char[i]]] = 1

vocab_size = len(char_dict)

print(X.shape, vocab_size)

# del questions, text, sentences, next_char
del text, sentences, next_char

def build():
  input = Input(shape=(MAX_LEN, vocab_size))
  
  # x = Bidirectional(GRU(128))(input)
  # x = Bidirectional(LSTM(128, return_sequences=True))(input)
  x = LSTM(128)(input)

  x = Flatten()(x)

  x = Dropout(.1)(x)

  # x = Dense(vocab_size, activation='relu', kernel_regularizer=l2(0.01))(x)
  x = Dense(vocab_size, activation='softmax')(x)

  model = Model(input, x)

  model.compile(loss='categorical_crossentropy', optimizer=RMSprop(.01), metrics=['accuracy'])
  model.summary()

  return model

from tensorflow.keras.utils import plot_model

model = build()
plot_model(model)

# import pickle
# import heapq

save_checkpt = ModelCheckpoint(filepath='./autoc.h5', save_best_only=True, monitor='val_loss')

# start = time()
model = build()

# text = 'the'

# model = load_model('./keras_model_gita.h5')

# chars = ['\n', ' ', '!', '"', '$', '%', '&', "'", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', '@', '[', '\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '£', '¥', '§', '©', '®', '°', '»', 'é', '—', '‘', '’', '“', '”', '€', 'ﬁ', 'ﬂ']

# x = np.zeros((1, 40, len(chars)))

# with open('char_indices.p', 'rb') as f:
#   char_indices = pickle.load(f)

# for t, char in enumerate(text):
#   x[0, t, char_indices[char]] = 1


# preds = model.predict(x, verbose=0)[0]

# preds = np.asarray(preds).astype('float64')
# preds = np.log(preds)
# exp_preds = np.exp(preds)
# preds = exp_preds / np.sum(exp_preds)

# pred = heapq.nlargest(3, range(len(preds)), preds.take)

# print(pred)

# with open('indices_char.p', 'rb') as f:
#   indices_char = pickle.load(f)

# for i in pred:
#   print(indices_char[i])

print(X.shape)
print(y.shape)

history = model.fit(X, y, batch_size=32, epochs=20, callbacks=[save_checkpt], validation_split=.1)

# show_grid_results(start)

model.save('autoc2.h5')

import ast
with open('dict.txt', 'r') as f:
    char_dict = f.read()
    char_dict = ast.literal_eval(char_dict)

    print(char_dict)

import heapq
import pickle
import sys

model = load_model('./autoc2.h5')
# model = load_model('./keras_model_gita.h5')


'''
Prediction with single letter
'''

msg = "sa"

AMT = 5

if len(msg) < AMT:
  n = AMT - len(msg)
  msg = n*' ' + msg

msg_ = msg[:5].lower()

indices_char = dict((j, i) for i, j in char_dict.items())

# with open('char_indices.p', 'rb') as f:
#   char_indices = pickle.load(f)

# with open('indices_char.p', 'rb') as f:
#   indices_char = pickle.load(f)

# print(char_indices)


def prepare_input(text):
    x = np.zeros((1, AMT, len(char_lst)))
    for t, char in enumerate(text):
       x[0, t, char_dict[char]] = 1.
    return x

def sample(preds, top_n=3):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds)
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)

    return heapq.nlargest(top_n, range(len(preds)), preds.take)

def predict_completion(text):
    original_text = text
    generated = text
    completion = ''

    repeated = []
    while True:
        x = prepare_input(text)
        preds = model.predict(x, verbose=0)[0]
        
        # print(preds)
        # print(preds.shape)

        next_index = sample(preds, top_n=1)[0]
        next_char = indices_char[next_index]

        repeated.append(next_char)
        if len(repeated) == 3 and repeated.count(repeated[0]) == len(repeated):
          return

        text = text[1:] + next_char
        completion += next_char
        
        if len(original_text + completion) + 2 > len(original_text) and next_char == ' ':
            return completion

def predict_completions(text, n=3):
    x = prepare_input(text)
    preds = model.predict(x, verbose=0)[0]
    next_indices = sample(preds, n)
    try:
      return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]
    except:
      pass


result = predict_completions(msg_, 5)

print(result)

# quotes = [
#     "It is not a lack of love, but a lack of friendship that makes unhappy marriages.",
#     "That which does not kill us makes us stronger.",
#     "I'm not upset that you lied to me, I'm upset that from now on I can't believe you.",
#     "And those who were seen dancing were thought to be insane by those who could not hear the music.",
#     "It is hard enough to remember my opinions, without also remembering my reasons for them!"
# ] 
# for q in quotes:
#     seq = q[:40].lower()
#     print(seq)
#     print(predict_completions(seq, 5))
#     print()

# import heapq

# msg = 'the'

# pred = model.predict(msg)
# print(pred)

# sys.exit()

# rev_char_dict = {char_lst.index(char): char for char in char_lst}

# def prepare_input(text):
#     x = np.zeros((1, MAX_LEN, len(char_lst)))
#     for t, char in enumerate(text):
#        x[0, t, char_dict[char]] = 1.
#     return x
  
# def sample(preds, top_n=3):
#    preds = np.asarray(preds).astype('float64')
#    preds = np.log(preds)
#    exp_preds = np.exp(preds)
#    preds = exp_preds / np.sum(exp_preds)
#    return heapq.nlargest(top_n, range(len(preds)), preds.take)

# def predict_completion(text):
#     original_text = text
#     generated = text
#     completion = ''
#     while True:
#         x = prepare_input(text)
#         preds = model.predict(x, verbose=0)[0]
#         next_index = sample(preds, top_n=1)[0]
#         next_char = rev_char_dict[next_index]
#         text = text[1:] + next_char
#         completion += next_char
        
#         if len(original_text + completion) + 2 > len(original_text) and next_char == ' ':
#             return completion

# def predict_completions(text, n=3):

#     x = prepare_input(text)

#     preds = model.predict(x, verbose=0)[0]

#     next_indices = sample(preds, n)

#     return [rev_char_dict[idx] + predict_completion(text[1:] + rev_char_dict[idx]) for idx in next_indices]

# seq = msg[:60].lower()
# print(seq)
# print(predict_completions(seq, 5))

# letters = list(msg)

# arr = np.zeros((1, MAX_LEN, vocab_size))

# for i, c in enumerate(letters):
#   arr[0][i][char_dict[c]] = 1
#   # arr = np.expand_dims(arr, 0)
#   # arr = arr[np.newaxis, :]
  
#   # if i == 0:
#   #   total_l = arr
#   # else:
#   #   total_l = np.concatenate([total_l, arr])

# print(arr)
# print(arr.shape)

# arr = np.concatenate([arr, arr2])
# print(arr)
# print(arr.shape)

# full = []
# for i in range(40):
#   i = random_predict(n_model.predict(arr)[0], .9)
#   full.append(i)

# reverse_char_dict = {value: key for key, value in char_dict.items()}
# gen = ''
# for l in full:
#   index = np.argmax(l)

#   # if reverse_char_dict[index] == " ":
#   #   break

#   # print(reverse_char_dict[index])
#   gen += reverse_char_dict[index]

# print(gen)

model.save('./model.h5')

acc = history.history['accuracy']
loss = history.history['loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training accuracy')
plt.title('Training accuracy')
plt.figure()
plt.plot(epochs, loss, 'b', label='Training loss')
plt.title('Training loss')
plt.legend()
plt.show()

test_file = open('test.txt')

corpus = test_file.read().lower().split('\n')[0:20]

for i, line in enumerate(corpus):
  nline = line.split(';')[0]
  corpus[i] = nline

tokenizer.fit_on_texts(corpus)

max_len_test = len(tokenizer.word_index) - 1

seed_text = ''

for line in corpus:
  token_list = tokenizer.texts_to_sequences([line])[0]

  token_list = pad_sequences([token_list], maxlen=max_len_test - 1, padding='pre')

  predicted = model.predict(token_list, verbose=0)

  predicted = np.argmax(predicted)

  output_word = ''

  for word, index in tokenizer.word_index.items():
        if index == predicted:
            output_word = word
            break

  seed_text += " " + output_word

print(seed_text)

